{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/argen7um/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/argen7um/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from collections import Counter\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, nb_workers = 8)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_and_score(pathes):\n",
    "    text_score = []\n",
    "    for path in pathes:\n",
    "        file_names = os.listdir(path)\n",
    "        for file_name in file_names:\n",
    "            with open(f'{path}/{file_name}', 'r') as file:\n",
    "                text = file.read()\n",
    "                score = int(file_name.split('.')[0].split('_')[-1])\n",
    "            text_score.append([text, score])\n",
    "    return text_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'../data/aclImdb'\n",
    "corpus = get_text_and_score([f'{path}/train/pos/', f'{path}/train/neg/', \n",
    "                             f'{path}/test/pos/', f'{path}/test/neg/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(corpus, columns=('data', 'target'))\n",
    "\n",
    "texts_count = 50000\n",
    "dataset = dataset.iloc[:texts_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_regex(text):\n",
    "    text = re.sub('\\\"{2,3}', '', text)\n",
    "    text = re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "    text = re.sub(r'[.,!?-]', '', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [word for word in set(stopwords.words('english')) if word not in ('no', 'not')]\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lenth = 1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_regex(text):\n",
    "    text = re.sub('\\\"{2,3}', '', text)\n",
    "    text = re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "    text = re.sub(r'[.,!?-]', '', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def split_words_by_space(text):\n",
    "    return text.split()\n",
    "\n",
    "def make_lower(word_list):\n",
    "    return list(map(lambda t: t.lower(), word_list))\n",
    "\n",
    "def lemmatize(word_list):\n",
    "    return list(map(lemmatizer.lemmatize, word_list))\n",
    "\n",
    "def remove_stop_words1(word_list):\n",
    "    text = join_word_list(word_list)\n",
    "    text = re.sub(rex_to_rem_stopwords, ' ', text)\n",
    "    return split_words_by_space(text)\n",
    "\n",
    "\n",
    "def add_bos_tag(word_list):\n",
    "    word_list.insert(0, '<bos>')\n",
    "    return word_list\n",
    "\n",
    "def limit_sequence(word_list, max_lenth):\n",
    "    return word_list[:max_lenth]\n",
    "\n",
    "def join_word_list(word_list):\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    word_list = tokenize_text(text)\n",
    "    lower_word_list = make_lower(word_list)\n",
    "    lemmatized_word_list = make_lower(lower_word_list)\n",
    "    word_list_with_bos = add_bos_tag(remove_stop_words1(lemmatized_word_list))\n",
    "    preprocessed_text = join_word_list(limit_sequence(word_list_with_bos, 1000))\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e46206eda9d4799a6b03d7c78a70faa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=6250), Label(value='0 / 6250'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "count_words = Counter(nltk.flatten(list(dataset['data'][:].parallel_apply(lambda a: lemmatize(make_lower(tokenize_text(clean_regex(a))))))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_word = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = list(set(stopwords).union(set(word for word in count_words if count_words[word] >= threshold_word)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "rex_to_rem_stopwords = '|'.join(f'\\s{word}\\s' for word in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a249c8a817fc42b49f7463590faa4730",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=6250), Label(value='0 / 6250'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b985ff7deb34a80b2f913fcca6aabe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=6250), Label(value='0 / 6250'))), …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0        <bos> this is great story s everything slavery...\n",
       "1        <bos> a years a got one his friends video the ...\n",
       "2        <bos> i this a but s fact you not it you from ...\n",
       "3        <bos> a movie family sacrifice and macy such g...\n",
       "4        <bos> one the merrie melodies cartoons enterta...\n",
       "                               ...                        \n",
       "49995    <bos> if are bit and to some you try one waste...\n",
       "49996    <bos> r is of only films ve where can say simp...\n",
       "49997    <bos> ruggero is credited inventing cannibal w...\n",
       "49998    <bos> stick is of brief of films paired ray ol...\n",
       "49999    <bos> i t i tried watch filth an bhorror fan w...\n",
       "Name: preprocessed text, Length: 50000, dtype: object"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['cleaned text'] = dataset['data'].parallel_apply(clean_regex)\n",
    "dataset['preprocessed text'] = dataset['cleaned text'].parallel_apply(preprocess_text)\n",
    "dataset['preprocessed text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'processed text'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3653\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3652\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_engine\u001b[39m.\u001b[39;49mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n",
      "File \u001b[0;32m~/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandas/_libs/index.pyx:147\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandas/_libs/index.pyx:176\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'processed text'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[56], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ngrams(dataset[\u001b[39m\"\u001b[39;49m\u001b[39mprocessed text\u001b[39;49m\u001b[39m\"\u001b[39;49m])\n",
      "File \u001b[0;32m~/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandas/core/frame.py:3761\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3759\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcolumns\u001b[39m.\u001b[39mnlevels \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   3760\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 3761\u001b[0m indexer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcolumns\u001b[39m.\u001b[39;49mget_loc(key)\n\u001b[1;32m   3762\u001b[0m \u001b[39mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   3763\u001b[0m     indexer \u001b[39m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m~/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandas/core/indexes/base.py:3655\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3653\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine\u001b[39m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3654\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m \u001b[39mas\u001b[39;00m err:\n\u001b[0;32m-> 3655\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyError\u001b[39;00m(key) \u001b[39mfrom\u001b[39;00m \u001b[39merr\u001b[39;00m\n\u001b[1;32m   3656\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mTypeError\u001b[39;00m:\n\u001b[1;32m   3657\u001b[0m     \u001b[39m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3658\u001b[0m     \u001b[39m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3659\u001b[0m     \u001b[39m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3660\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'processed text'"
     ]
    }
   ],
   "source": [
    "ngrams(dataset[\"processed text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>This show is a great history story. It's has e...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A few years ago, a friend got from one of his ...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I like this movie a lot, but it's a fact, that...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A brilliant movie about family, guilt, sacrifi...</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>One of the many Merrie Melodies cartoons that ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49995</th>\n",
       "      <td>If you are a bit masochistic and like to waste...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49996</th>\n",
       "      <td>'R Xmas is one of the only films I've seen whe...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49997</th>\n",
       "      <td>Ruggero Deodato is often credited for inventin...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49998</th>\n",
       "      <td>\"Stick Around\" is one of the brief series of f...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49999</th>\n",
       "      <td>I can't believe I even tried to watch this fil...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>50000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    data  target\n",
       "0      This show is a great history story. It's has e...      10\n",
       "1      A few years ago, a friend got from one of his ...      10\n",
       "2      I like this movie a lot, but it's a fact, that...      10\n",
       "3      A brilliant movie about family, guilt, sacrifi...       9\n",
       "4      One of the many Merrie Melodies cartoons that ...       7\n",
       "...                                                  ...     ...\n",
       "49995  If you are a bit masochistic and like to waste...       1\n",
       "49996  'R Xmas is one of the only films I've seen whe...       4\n",
       "49997  Ruggero Deodato is often credited for inventin...       4\n",
       "49998  \"Stick Around\" is one of the brief series of f...       4\n",
       "49999  I can't believe I even tried to watch this fil...       1\n",
       "\n",
       "[50000 rows x 2 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

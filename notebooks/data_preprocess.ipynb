{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO: Pandarallel will run on 8 workers.\n",
      "INFO: Pandarallel will use Memory file system to transfer data between the main process and workers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/argen7um/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/argen7um/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from tensorflow.keras.utils import pad_sequences\n",
    "from collections import Counter\n",
    "from pandarallel import pandarallel\n",
    "\n",
    "pandarallel.initialize(progress_bar=True, nb_workers = 8)\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_and_score(pathes):\n",
    "    text_score = []\n",
    "    for path in pathes:\n",
    "        file_names = os.listdir(path)\n",
    "        for file_name in file_names:\n",
    "            with open(f'{path}/{file_name}', 'r') as file:\n",
    "                text = file.read()\n",
    "                score = int(file_name.split('.')[0].split('_')[-1])\n",
    "            text_score.append([text, score])\n",
    "    return text_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = r'../data/aclImdb'\n",
    "corpus = get_text_and_score([f'{path}/train/pos/', f'{path}/train/neg/', \n",
    "                             f'{path}/test/pos/', f'{path}/test/neg/'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.DataFrame(corpus, columns=('data', 'target'))\n",
    "\n",
    "texts_count = 50000\n",
    "dataset = dataset.iloc[:texts_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_regex(text):\n",
    "    text = re.sub('\\\"{2,3}', '', text)\n",
    "    text = re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "    text = re.sub(r'[.,!?-]', '', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = [word for word in set(stopwords.words('english')) if word not in ('no', 'not')]\n",
    "rex_to_rem_stopwords = '|'.join(f'\\s{word}\\s' for word in stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_lenth = 1_000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_regex(text):\n",
    "    text = re.sub('\\\"{2,3}', '', text)\n",
    "    text = re.sub(r'[\\\"\\#\\$\\%\\&\\'\\(\\)\\*\\+\\/\\:\\;\\<\\=\\>\\@\\[\\\\\\]\\^\\_\\`\\{\\|\\}\\~]', ' ', text)\n",
    "    text = re.sub(r'[.,!?-]', '', text)\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = re.sub(r' +', ' ', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7f]', r'', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "def split_words_by_space(text):\n",
    "    return text.split()\n",
    "\n",
    "def make_lower(word_list):\n",
    "    return list(map(lambda t: t.lower(), word_list))\n",
    "\n",
    "def lemmatize(word_list):\n",
    "    return list(map(lemmatizer.lemmatize, word_list))\n",
    "\n",
    "def remove_stop_words1(word_list):\n",
    "    text = join_word_list(word_list)\n",
    "    text = re.sub(rex_to_rem_stopwords, ' ', text)\n",
    "    return split_words_by_space(text)\n",
    "\n",
    "\n",
    "def add_bos_tag(word_list):\n",
    "    word_list.insert(0, '<bos>')\n",
    "    return word_list\n",
    "\n",
    "def limit_sequence(word_list, max_lenth):\n",
    "    return word_list[:max_lenth]\n",
    "\n",
    "def join_word_list(word_list):\n",
    "    return ' '.join(word_list)\n",
    "\n",
    "def preprocess_text(text):\n",
    "    word_list = tokenize_text(text)\n",
    "    lower_word_list = make_lower(word_list)\n",
    "    lemmatized_word_list = make_lower(lower_word_list)\n",
    "    word_list_with_bos = add_bos_tag(remove_stop_words1(lemmatized_word_list))\n",
    "    preprocessed_text = join_word_list(limit_sequence(word_list_with_bos, 1000))\n",
    "    \n",
    "    return preprocessed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f90d35032a9d4ed1a5c7c4a65ba8c396",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=6250), Label(value='0 / 6250'))), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandarallel/core.py\", line 95, in __call__\n    result = self.work_function(\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandarallel/data_types/series.py\", line 26, in work\n    return data.apply(\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandas/core/series.py\", line 4630, in apply\n    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandas/core/apply.py\", line 1025, in apply\n    return self.apply_standard()\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandas/core/apply.py\", line 1076, in apply_standard\n    mapped = lib.map_infer(\n  File \"pandas/_libs/lib.pyx\", line 2834, in pandas._libs.lib.map_infer\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandarallel/progress_bars.py\", line 214, in closure\n    return user_defined_function(\n  File \"/tmp/ipykernel_12382/3746553212.py\", line 2, in <lambda>\n    count_words = Counter(nltk.flatten(list(dataset['data'].parallel_apply(lambda a: clean_regex(tokenize_text(make_lower(a)))))))\n  File \"/tmp/ipykernel_12382/485903776.py\", line 13, in tokenize_text\n    return word_tokenize(text)\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 129, in word_tokenize\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 107, in sent_tokenize\n    return tokenizer.tokenize(text)\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1281, in tokenize\n    return list(self.sentences_from_text(text, realign_boundaries))\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1341, in sentences_from_text\n    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1341, in <listcomp>\n    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1329, in span_tokenize\n    for sentence in slices:\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1459, in _realign_boundaries\n    for sentence1, sentence2 in _pair_iter(slices):\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 321, in _pair_iter\n    prev = next(iterator)\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1431, in _slices_from_text\n    for match, context in self._match_potential_end_contexts(text):\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1395, in _match_potential_end_contexts\n    for match in self._lang_vars.period_context_re().finditer(text):\nTypeError: expected string or bytes-like object\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m threshold_count \u001b[39m=\u001b[39m \u001b[39m15\u001b[39m\n\u001b[0;32m----> 2\u001b[0m count_words \u001b[39m=\u001b[39m Counter(nltk\u001b[39m.\u001b[39mflatten(\u001b[39mlist\u001b[39m(dataset[\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mparallel_apply(\u001b[39mlambda\u001b[39;49;00m a: clean_regex(tokenize_text(make_lower(a)))))))\n\u001b[1;32m      4\u001b[0m stopwords \u001b[39m=\u001b[39m \u001b[39mset\u001b[39m(stopwords) \u001b[39m+\u001b[39m \u001b[39mset\u001b[39m(word \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m count_words \u001b[39mif\u001b[39;00m count_words[word] \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m threshold_count)\n\u001b[1;32m      6\u001b[0m \u001b[39m# stopwords = sorted(list(stopwords))\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# vocabulary = [word for word in count_words if count_words[word] > threshold_count and word not in stopwords]\u001b[39;00m\n",
      "File \u001b[0;32m~/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandarallel/core.py:333\u001b[0m, in \u001b[0;36mparallelize_with_memory_file_system.<locals>.closure\u001b[0;34m(data, user_defined_function, *user_defined_function_args, **user_defined_function_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[39mreturn\u001b[39;00m wrapped_reduce_function(\n\u001b[1;32m    326\u001b[0m         (Path(output_file\u001b[39m.\u001b[39mname) \u001b[39mfor\u001b[39;00m output_file \u001b[39min\u001b[39;00m output_files),\n\u001b[1;32m    327\u001b[0m         reduce_extra,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    329\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEOFError\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39m# Loading the files failed, this most likely means that there\u001b[39;00m\n\u001b[1;32m    331\u001b[0m     \u001b[39m# was some error during processing and the files were never\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[39m# saved at all.\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     results_promise\u001b[39m.\u001b[39;49mget()\n\u001b[1;32m    335\u001b[0m     \u001b[39m# If the above statement does not raise an exception, that\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     \u001b[39m# means the multiprocessing went well and we want to re-raise\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[39m# the original EOFError.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "threshold_count = 15\n",
    "count_words = Counter(nltk.flatten(list(dataset['data'].parallel_apply(lambda a: clean_regex(tokenize_text(make_lower(a)))))))\n",
    "\n",
    "stopwords = set(stopwords) + set(word for word in count_words if count_words[word] <= threshold_count)\n",
    "\n",
    "# stopwords = sorted(list(stopwords))\n",
    "# vocabulary = [word for word in count_words if count_words[word] > threshold_count and word not in stopwords]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e4d7cb9be53d4b9692b5e379eff3f474",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(IntProgress(value=0, description='0.00%', max=6250), Label(value='0 / 6250'))), â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 125, in worker\n    result = (True, func(*args, **kwds))\n  File \"/usr/lib/python3.10/multiprocessing/pool.py\", line 51, in starmapstar\n    return list(itertools.starmap(args[0], args[1]))\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandarallel/core.py\", line 95, in __call__\n    result = self.work_function(\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandarallel/data_types/series.py\", line 26, in work\n    return data.apply(\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandas/core/series.py\", line 4630, in apply\n    return SeriesApply(self, func, convert_dtype, args, kwargs).apply()\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandas/core/apply.py\", line 1025, in apply\n    return self.apply_standard()\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandas/core/apply.py\", line 1076, in apply_standard\n    mapped = lib.map_infer(\n  File \"pandas/_libs/lib.pyx\", line 2834, in pandas._libs.lib.map_infer\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandarallel/progress_bars.py\", line 214, in closure\n    return user_defined_function(\n  File \"/tmp/ipykernel_12382/2072859099.py\", line 1, in <lambda>\n    dataset['data'].parallel_apply(lambda a: clean_regex(tokenize_text(make_lower(a))))\n  File \"/tmp/ipykernel_12382/485903776.py\", line 13, in tokenize_text\n    return word_tokenize(text)\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 129, in word_tokenize\n    sentences = [text] if preserve_line else sent_tokenize(text, language)\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/__init__.py\", line 107, in sent_tokenize\n    return tokenizer.tokenize(text)\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1281, in tokenize\n    return list(self.sentences_from_text(text, realign_boundaries))\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1341, in sentences_from_text\n    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1341, in <listcomp>\n    return [text[s:e] for s, e in self.span_tokenize(text, realign_boundaries)]\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1329, in span_tokenize\n    for sentence in slices:\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1459, in _realign_boundaries\n    for sentence1, sentence2 in _pair_iter(slices):\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 321, in _pair_iter\n    prev = next(iterator)\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1431, in _slices_from_text\n    for match, context in self._match_potential_end_contexts(text):\n  File \"/home/argen7um/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/nltk/tokenize/punkt.py\", line 1395, in _match_potential_end_contexts\n    for match in self._lang_vars.period_context_re().finditer(text):\nTypeError: expected string or bytes-like object\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dataset[\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mparallel_apply(\u001b[39mlambda\u001b[39;49;00m a: clean_regex(tokenize_text(make_lower(a))))\n",
      "File \u001b[0;32m~/Desktop/code/green_atom/mlweb/green_atom/venv/lib/python3.10/site-packages/pandarallel/core.py:333\u001b[0m, in \u001b[0;36mparallelize_with_memory_file_system.<locals>.closure\u001b[0;34m(data, user_defined_function, *user_defined_function_args, **user_defined_function_kwargs)\u001b[0m\n\u001b[1;32m    325\u001b[0m     \u001b[39mreturn\u001b[39;00m wrapped_reduce_function(\n\u001b[1;32m    326\u001b[0m         (Path(output_file\u001b[39m.\u001b[39mname) \u001b[39mfor\u001b[39;00m output_file \u001b[39min\u001b[39;00m output_files),\n\u001b[1;32m    327\u001b[0m         reduce_extra,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    329\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mEOFError\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[39m# Loading the files failed, this most likely means that there\u001b[39;00m\n\u001b[1;32m    331\u001b[0m     \u001b[39m# was some error during processing and the files were never\u001b[39;00m\n\u001b[1;32m    332\u001b[0m     \u001b[39m# saved at all.\u001b[39;00m\n\u001b[0;32m--> 333\u001b[0m     results_promise\u001b[39m.\u001b[39;49mget()\n\u001b[1;32m    335\u001b[0m     \u001b[39m# If the above statement does not raise an exception, that\u001b[39;00m\n\u001b[1;32m    336\u001b[0m     \u001b[39m# means the multiprocessing went well and we want to re-raise\u001b[39;00m\n\u001b[1;32m    337\u001b[0m     \u001b[39m# the original EOFError.\u001b[39;00m\n\u001b[1;32m    338\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "File \u001b[0;32m/usr/lib/python3.10/multiprocessing/pool.py:774\u001b[0m, in \u001b[0;36mApplyResult.get\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    772\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n\u001b[1;32m    773\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 774\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_value\n",
      "\u001b[0;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "dataset['data'].parallel_apply(lambda a: clean_regex(tokenize_text(make_lower(a))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
